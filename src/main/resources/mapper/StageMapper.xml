<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.spark.insight.mapper.StageMapper">

    <!-- 预计算 Stage 的聚合指标 -->
    <update id="updateStageMetrics">
        UPDATE stages s
        SET 
            gc_time_sum = m.total_gc,
            tasks_duration_sum = m.total_duration,
            executor_deserialize_time_sum = m.total_deser,
            result_serialization_time_sum = m.total_ser,
            getting_result_time_sum = m.total_get_res,
            scheduler_delay_sum = m.total_delay,
            peak_execution_memory_max = m.max_peak_mem,
            peak_execution_memory_sum = m.total_peak_mem,
            memory_bytes_spilled_sum = m.total_mem_spill,
            disk_bytes_spilled_sum = m.total_disk_spill,
            shuffle_write_time_sum = m.total_sw_time,
            num_completed_tasks = m.done_tasks,
            num_failed_tasks = m.failed_tasks,
            input_bytes = m.total_input,
            input_records = m.total_input_records,
            output_bytes = m.total_output,
            output_records = m.total_output_records,
            shuffle_read_bytes = m.total_shuffle_read,
            shuffle_read_records = m.total_shuffle_read_records,
            shuffle_write_bytes = m.total_shuffle_write,
            shuffle_write_records = m.total_shuffle_write_records,
            duration_p50 = m.p50,
            duration_p95 = m.p95,
            duration_p99 = m.p99,
            max_task_duration = m.max_dur,
            is_skewed = (CASE WHEN m.max_dur > m.p50 * 2 THEN TRUE ELSE FALSE END),
            locality_summary = m_loc.loc_summary,
            performance_score = (
                100.0 - (
                    COALESCE((m.total_gc * 100.0 / NULLIF(m.total_duration, 0)), 0) * 0.15 +
                    COALESCE((m.total_sw_time / 1000000.0 * 100.0 / NULLIF(m.total_duration, 0)), 0) * 0.15 +
                    COALESCE((m.total_fetch_wait / 1000000.0 * 100.0 / NULLIF(m.total_duration, 0)), 0) * 0.15 +
                    CASE WHEN m.p50 > 0 THEN LEAST(100, (m.max_dur * 1.0 / m.p50 - 1) * 50) ELSE 0 END * 0.15 +
                    CASE WHEN m.total_disk_spill > 0 THEN 100 ELSE 0 END * 0.15 +
                    (100.0 - LEAST(100, COALESCE((m.total_cpu_time / 1000000.0 * 100.0 / NULLIF(m.total_run_time, 0)), 100))) * 0.10 +
                    COALESCE((m.total_delay * 100.0 / NULLIF(m.total_duration, 0)), 0) * 0.05 +
                    COALESCE(((m.total_ser + m.total_deser) * 100.0 / NULLIF(m.total_duration, 0)), 0) * 0.05 +
                    COALESCE((m.total_get_res * 100.0 / NULLIF(m.total_duration, 0)), 0) * 0.05
                )
            ),
            diagnosis_info = json_array(
                json_object('dimension', 'GC Impact', 'score', CAST(100 - COALESCE((m.total_gc * 100.0 / NULLIF(m.total_duration, 0)), 0) AS INTEGER)),
                json_object('dimension', 'Shuffle Write Impact', 'score', CAST(100 - COALESCE((m.total_sw_time / 1000000.0 * 100.0 / NULLIF(m.total_duration, 0)), 0) AS INTEGER)),
                json_object('dimension', 'Shuffle Read Blocked', 'score', CAST(100 - COALESCE((m.total_fetch_wait / 1000000.0 * 100.0 / NULLIF(m.total_duration, 0)), 0) AS INTEGER)),
                json_object('dimension', 'I/O Wait', 'score', CAST(LEAST(100, COALESCE((m.total_cpu_time / 1000000.0 * 100.0 / NULLIF(m.total_run_time, 0)), 100)) AS INTEGER)),
                json_object('dimension', 'Serialization Impact', 'score', CAST(100 - COALESCE(((m.total_ser + m.total_deser) * 100.0 / NULLIF(m.total_duration, 0)), 0) AS INTEGER)),
                json_object('dimension', 'Result Fetching', 'score', CAST(100 - COALESCE((m.total_get_res * 100.0 / NULLIF(m.total_duration, 0)), 0) AS INTEGER)),
                json_object('dimension', 'Scheduler Delay Impact', 'score', CAST(100 - COALESCE((m.total_delay * 100.0 / NULLIF(m.total_duration, 0)), 0) AS INTEGER)),
                json_object('dimension', 'Data Skew', 'score', CAST(100 - CASE WHEN m.p50 > 0 THEN LEAST(100, (m.max_dur * 1.0 / m.p50 - 1) * 50) ELSE 0 END AS INTEGER)),
                json_object('dimension', 'Disk Spill', 'score', CASE WHEN m.total_disk_spill > 0 THEN 0 ELSE 100 END)
            )
        FROM (
            SELECT 
                base.app_id, 
                base.stage_id,
                base.attempt_id,
                string_agg(k || ': ' || v, ', ') as loc_summary
            FROM (
                SELECT app_id, stage_id, attempt_id, locality as k, count(*) as v
                FROM tasks
                WHERE app_id = #{appId} AND locality IS NOT NULL
                GROUP BY app_id, stage_id, attempt_id, locality
            ) loc_agg
            RIGHT JOIN (
                SELECT DISTINCT app_id, stage_id, attempt_id FROM tasks WHERE app_id = #{appId}
            ) base ON loc_agg.app_id = base.app_id AND loc_agg.stage_id = base.stage_id AND loc_agg.attempt_id = base.attempt_id
            GROUP BY base.app_id, base.stage_id, base.attempt_id
        ) m_loc
        JOIN (
            SELECT 
                app_id, 
                stage_id,
                attempt_id,
                count(case when status = 'SUCCESS' or status = 'SUCCEEDED' then 1 end) as done_tasks,
                count(case when status = 'FAILED' then 1 end) as failed_tasks,
                sum(gc_time) as total_gc,
                sum(duration) as total_duration,
                sum(executor_deserialize_time) as total_deser,
                sum(result_serialization_time) as total_ser,
                sum(getting_result_time) as total_get_res,
                sum(scheduler_delay) as total_delay,
                max(peak_execution_memory) as max_peak_mem,
                sum(peak_execution_memory) as total_peak_mem,
                sum(memory_bytes_spilled) as total_mem_spill,
                sum(disk_bytes_spilled) as total_disk_spill,
                sum(shuffle_write_time) as total_sw_time,
                sum(shuffle_fetch_wait_time) as total_fetch_wait,
                sum(executor_cpu_time) as total_cpu_time,
                sum(executor_run_time) as total_run_time,
                sum(input_bytes) as total_input,
                sum(input_records) as total_input_records,
                sum(output_bytes) as total_output,
                sum(output_records) as total_output_records,
                sum(shuffle_read_bytes) as total_shuffle_read,
                sum(shuffle_read_records) as total_shuffle_read_records,
                sum(shuffle_write_bytes) as total_shuffle_write,
                sum(shuffle_write_records) as total_shuffle_write_records,
                approx_quantile(duration, 0.5) as p50,
                approx_quantile(duration, 0.95) as p95,
                approx_quantile(duration, 0.99) as p99,
                max(duration) as max_dur
            FROM tasks
            WHERE app_id = #{appId}
            GROUP BY app_id, stage_id, attempt_id
        ) m ON m.app_id = m_loc.app_id AND m.stage_id = m_loc.stage_id AND m.attempt_id = m_loc.attempt_id
        WHERE s.app_id = m.app_id AND s.stage_id = m.stage_id AND s.attempt_id = m.attempt_id
    </update>

    <delete id="deleteStageStats">
        DELETE FROM stage_statistics WHERE app_id = #{appId}
    </delete>

    <!-- 计算详细的 Task 分布统计并存入 stage_statistics 表 -->
    <insert id="insertTaskStats">
        INSERT INTO stage_statistics (id, app_id, stage_id, attempt_id, metric_name, min_value, p25, p50, p75, p95, max_value)
        WITH task_stats AS (
            SELECT 
                app_id, 
                stage_id,
                attempt_id,
                'duration' as metric_name,
                min(duration) as min_v,
                approx_quantile(duration, 0.25) as p25,
                approx_quantile(duration, 0.5) as p50,
                approx_quantile(duration, 0.75) as p75,
                approx_quantile(duration, 0.95) as p95,
                max(duration) as max_v
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'gc_time',
                min(gc_time), approx_quantile(gc_time, 0.25), approx_quantile(gc_time, 0.5), approx_quantile(gc_time, 0.75), approx_quantile(gc_time, 0.95), max(gc_time)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'memory_spill',
                min(memory_bytes_spilled), approx_quantile(memory_bytes_spilled, 0.25), approx_quantile(memory_bytes_spilled, 0.5), approx_quantile(memory_bytes_spilled, 0.75), approx_quantile(memory_bytes_spilled, 0.95), max(memory_bytes_spilled)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'disk_spill',
                min(disk_bytes_spilled), approx_quantile(disk_bytes_spilled, 0.25), approx_quantile(disk_bytes_spilled, 0.5), approx_quantile(disk_bytes_spilled, 0.75), approx_quantile(disk_bytes_spilled, 0.95), max(disk_bytes_spilled)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'shuffle_read',
                min(shuffle_read_bytes), approx_quantile(shuffle_read_bytes, 0.25), approx_quantile(shuffle_read_bytes, 0.5), approx_quantile(shuffle_read_bytes, 0.75), approx_quantile(shuffle_read_bytes, 0.95), max(shuffle_read_bytes)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'shuffle_read_records',
                min(shuffle_read_records), approx_quantile(shuffle_read_records, 0.25), approx_quantile(shuffle_read_records, 0.5), approx_quantile(shuffle_read_records, 0.75), approx_quantile(shuffle_read_records, 0.95), max(shuffle_read_records)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'shuffle_write',
                min(shuffle_write_bytes), approx_quantile(shuffle_write_bytes, 0.25), approx_quantile(shuffle_write_bytes, 0.5), approx_quantile(shuffle_write_bytes, 0.75), approx_quantile(shuffle_write_bytes, 0.95), max(shuffle_write_bytes)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'shuffle_write_records',
                min(shuffle_write_records), approx_quantile(shuffle_write_records, 0.25), approx_quantile(shuffle_write_records, 0.5), approx_quantile(shuffle_write_records, 0.75), approx_quantile(shuffle_write_records, 0.95), max(shuffle_write_records)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'task_deserialization_time',
                min(executor_deserialize_time), approx_quantile(executor_deserialize_time, 0.25), approx_quantile(executor_deserialize_time, 0.5), approx_quantile(executor_deserialize_time, 0.75), approx_quantile(executor_deserialize_time, 0.95), max(executor_deserialize_time)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'result_serialization_time',
                min(result_serialization_time), approx_quantile(result_serialization_time, 0.25), approx_quantile(result_serialization_time, 0.5), approx_quantile(result_serialization_time, 0.75), approx_quantile(result_serialization_time, 0.95), max(result_serialization_time)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'getting_result_time',
                min(getting_result_time), approx_quantile(getting_result_time, 0.25), approx_quantile(getting_result_time, 0.5), approx_quantile(getting_result_time, 0.75), approx_quantile(getting_result_time, 0.95), max(getting_result_time)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'scheduler_delay',
                min(scheduler_delay), approx_quantile(scheduler_delay, 0.25), approx_quantile(scheduler_delay, 0.5), approx_quantile(scheduler_delay, 0.75), approx_quantile(scheduler_delay, 0.95), max(scheduler_delay)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'peak_execution_memory',
                min(peak_execution_memory), approx_quantile(peak_execution_memory, 0.25), approx_quantile(peak_execution_memory, 0.5), approx_quantile(peak_execution_memory, 0.75), approx_quantile(peak_execution_memory, 0.95), max(peak_execution_memory)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'input_bytes',
                min(input_bytes), approx_quantile(input_bytes, 0.25), approx_quantile(input_bytes, 0.5), approx_quantile(input_bytes, 0.75), approx_quantile(input_bytes, 0.95), max(input_bytes)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'input_records',
                min(input_records), approx_quantile(input_records, 0.25), approx_quantile(input_records, 0.5), approx_quantile(input_records, 0.75), approx_quantile(input_records, 0.95), max(input_records)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
            UNION ALL
            SELECT 
                app_id, stage_id, attempt_id, 'shuffle_write_time',
                min(shuffle_write_time), approx_quantile(shuffle_write_time, 0.25), approx_quantile(shuffle_write_time, 0.5), approx_quantile(shuffle_write_time, 0.75), approx_quantile(shuffle_write_time, 0.95), max(shuffle_write_time)
            FROM tasks WHERE app_id = #{appId} GROUP BY app_id, stage_id, attempt_id
        )
        SELECT 
            app_id || ':' || stage_id || ':' || attempt_id || ':' || metric_name,
            app_id, stage_id, attempt_id, metric_name, min_v, p25, p50, p75, p95, max_v
        FROM task_stats
    </insert>

    <!-- 按 Executor 聚合任务指标 -->
    <select id="getExecutorSummary" resultType="map">
        SELECT 
            executor_id as executorId,
            count(*) as taskCount,
            sum(executor_deserialize_time) as executorDeserializeTime,
            sum(duration) as duration,
            sum(gc_time) as gcTime,
            sum(result_serialization_time) as resultSerializationTime,
            sum(getting_result_time) as gettingResultTime,
            sum(scheduler_delay) as schedulerDelay,
            max(peak_execution_memory) as peakExecutionMemory,
            sum(memory_bytes_spilled) as memoryBytesSpilled,
            sum(disk_bytes_spilled) as diskBytesSpilled,
            sum(input_bytes) as inputBytes,
            sum(input_records) as inputRecords,
            sum(output_bytes) as outputBytes,
            sum(output_records) as outputRecords,
            sum(shuffle_read_bytes) as shuffleReadBytes,
            sum(shuffle_read_records) as shuffleReadRecords,
            sum(shuffle_write_bytes) as shuffleWriteBytes,
            sum(shuffle_write_records) as shuffleWriteRecords,
            sum(shuffle_write_time) as shuffleWriteTime
        FROM tasks
        WHERE app_id = #{appId} AND stage_id = #{stageId}
        <if test="attemptId != null">
            AND attempt_id = #{attemptId}
        </if>
        GROUP BY executor_id
        ORDER BY executor_id ASC
    </select>

    <select id="getJobExecutorSummary" resultType="map">
        SELECT 
            t.executor_id as executorId,
            count(*) as taskCount,
            sum(t.executor_deserialize_time) as executorDeserializeTime,
            sum(t.duration) as duration,
            sum(t.gc_time) as gcTime,
            sum(t.result_serialization_time) as resultSerializationTime,
            sum(t.getting_result_time) as gettingResultTime,
            sum(t.scheduler_delay) as schedulerDelay,
            max(t.peak_execution_memory) as peakExecutionMemory,
            sum(t.memory_bytes_spilled) as memoryBytesSpilled,
            sum(t.disk_bytes_spilled) as diskBytesSpilled,
            sum(t.input_bytes) as inputBytes,
            sum(t.input_records) as inputRecords,
            sum(t.output_bytes) as outputBytes,
            sum(t.output_records) as outputRecords,
            sum(t.shuffle_read_bytes) as shuffleReadBytes,
            sum(t.shuffle_read_records) as shuffleReadRecords,
            sum(t.shuffle_write_bytes) as shuffleWriteBytes,
            sum(t.shuffle_write_records) as shuffleWriteRecords,
            sum(t.shuffle_write_time) as shuffleWriteTime
        FROM tasks t
        JOIN stages s ON t.app_id = s.app_id AND t.stage_id = s.stage_id AND t.attempt_id = s.attempt_id
        WHERE t.app_id = #{appId} AND s.job_id = #{jobId}
        GROUP BY t.executor_id
        ORDER BY t.executor_id ASC
    </select>

</mapper>
